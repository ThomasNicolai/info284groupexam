{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:18.152271Z",
     "start_time": "2024-04-29T09:41:16.826204Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch xgboost tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:19.788068Z",
     "start_time": "2024-04-29T09:41:18.466397Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_absolute_error, mean_squared_error, r2_score, classification_report, f1_score, accuracy_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.src.saving import load_model\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('data.csv', on_bad_lines=\"warn\", delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our hypothetical use case, we imagine a government agency that is interested to predict how much endangered fish a given catch operation has caught.\n",
    "\n",
    "We used \"https://artsdatabanken.no/lister/rodlisteforarter/2021?Name=&SortBy=ScientificName&Meta=Visited&Meta=scroll_658&IsCheck=Area&Area=N&IsCheck=Category&Category=CR&Category=EN&IsCheck=SpeciesGroups&SpeciesGroups=Fisker&IsCheck=Insekter\" to find endangered species of fish. \n",
    "\n",
    "Three of these are included in the dataset:\n",
    "\n",
    "1. Vanlig Uer\n",
    "2. Blålange\n",
    "3. Ål\n",
    "\n",
    "Our strategy will be to trim unnecessary columns, turn species into boolean columns and have machine learning models predict the amount (weight) caught of the given species for each catch operation.\n",
    "\n",
    "<!-- A lot of the data in the columns needs to be converted into boolean and number values, and some needs to be omitted due to inaccuracies.  -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:21.851180Z",
     "start_time": "2024-04-29T09:41:21.790330Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.info()\n",
    "uer_df = df[df['Art FAO'] == \"Uer (vanlig)\"] \n",
    "blålange_df = df[df['Art FAO'] == \"Blålange\"] \n",
    "ål_df = df[df['Art FAO'] == \"Ål\"] \n",
    "print(\"Number of catches containing Uer: \", len(uer_df))\n",
    "print(\"Number of catches containing Blålange: \", len(blålange_df))\n",
    "print(\"Number of catches containing Ål: \", len(ål_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two catch operations containing Ål, we need more data in order to predict for this species.\n",
    "For Blålange and Uer we can move on. It's difficult to say beforehand wether or not 991 positive datapoints will be enough to accurately predict the Rundvekt of Blålange, but it's within the realm of possibilities.\n",
    " \n",
    "\n",
    "To begin trimming down the amount of data, we can safely drop columns where the data is already present in other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:24.340525Z",
     "start_time": "2024-04-29T09:41:24.225111Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(labels=[\"Meldingsdato\",\"Meldingsklokkeslett\",\"Startdato\",\"Startklokkeslett\", \"Stoppdato\", \"Stoppklokkeslett\", \"Lengdegruppe (kode)\", \"Lengdegruppe\"], axis=1)\n",
    "df = df.drop(labels=[\"Hovedart FAO (kode)\",\"Hovedart FAO\",\"Hovedart - FDIR (kode)\",\"Art - gruppe\",\"Art - gruppe (kode)\", \"Art - FDIR (kode)\", \"Art - FDIR\", \"Redskap FDIR (kode)\", \"Redskap FDIR\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that Start/Stop times are inconsistent. We therefore filter out Start/Stop times which do not contain ':'. This way we can trust the 'Varighet' column more.\n",
    "\n",
    "Furthermore, 'Fartøylengde', 'Bredde', 'Startposisjon bredde', 'Startposisjon lengde', 'Stopposisjon bredde', 'Stopposisjon lengde' is written with ',' and not '.'. We change ',' to '.' and cast to float datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:27.369736Z",
     "start_time": "2024-04-29T09:41:26.945238Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df[\"Starttidspunkt\"].str.contains(\":\")]\n",
    "df = df[df[\"Stopptidspunkt\"].str.contains(\":\")]\n",
    "\n",
    "castable_columns = [\"Fartøylengde\", \"Bredde\", \"Startposisjon bredde\", \"Startposisjon lengde\", \"Stopposisjon bredde\", \"Stopposisjon lengde\"]\n",
    "\n",
    "for column in castable_columns:\n",
    "    df[column] = df[column].str.replace(\",\", \".\").astype(\"float\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to convert the categorical column values we _do_ want to use into machine-readable data. In this case we use the built-in get_dummies function to create boolean columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:28.664014Z",
     "start_time": "2024-04-29T09:41:28.135795Z"
    }
   },
   "outputs": [],
   "source": [
    "species_columns = pd.get_dummies(df[\"Art FAO\"], dtype=\"float\") \n",
    "species_columns_multiplied = species_columns.multiply(df[\"Rundvekt\"], axis=\"index\") # Keep weight of fish\n",
    "\n",
    "endangered_species_1 = species_columns_multiplied[\"Uer (vanlig)\"]\n",
    "endangered_species_2 = species_columns_multiplied[\"Blålange\"]\n",
    "\n",
    "# Add endangered species and drop original columns.\n",
    "df = pd.concat([df,endangered_species_1 ], axis=1)\n",
    "df = pd.concat([df,endangered_species_2 ], axis=1)\n",
    "df = df.drop([\"Art FAO\", \"Art FAO (kode)\" ], axis=1)\n",
    "\n",
    "tool_columns = pd.get_dummies(df[\"Redskap FAO\"])\n",
    "df = pd.concat([df,tool_columns], axis=1) \n",
    "\n",
    "df = df.drop([\"Redskap FAO\", \"Redskap FAO (kode)\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the amount of parameters, we group the fishing tools into sensible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:30.638689Z",
     "start_time": "2024-04-29T09:41:30.459774Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups_to_combine = {\n",
    "    'DivBunntrål': ['Bunntrål (uspes)', 'Bunntrål, otter', 'Bunntrål, par', 'Bunntrål, reke', 'Skrape, mekanisert', 'Otter dobbeltrål'],\n",
    "    'DivSnurrevad': ['Snurrevad, skotsk', 'Snurrevad, dansk', 'Snurrevad'],\n",
    "    'DivSnurpenot/ringnot': ['Snurpenot/ringnot, to fartøy', 'Snurpenot/ringnot, et fartøy', 'Snurpenot/ringnot', 'Boat /vessel seines -Pair seines'],\n",
    "    'DivFlytetrål': ['Flytetrål, otter', 'Flytetrål, par', 'Flytetrål, reke', 'Flytetrål, uspesifisert'],\n",
    "    'DivGarn': ['Udefinert garn', 'Settegarn', 'Gillnets and entangling nets (unspec)*', 'Encircling gillnets*'],\n",
    "    'DivLine': ['Andre liner', 'Setteline', 'Dorg/harp/snik', 'Juksa/pilk, manuell'],\n",
    "    'Annet': ['Harpun,div.', 'Partrål, uspesifisert', 'Teiner', 'Udefinert trål', 'Annen trål (udefinert)']\n",
    "}\n",
    "\n",
    "for new_column, columns_to_group in groups_to_combine.items():\n",
    "    df[new_column] = df[columns_to_group].any(axis=1)\n",
    "    df.drop(columns=columns_to_group, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When grouping the different catch operations, it's important to be precise when telling pandas how to process the different rows.\n",
    "\n",
    "The data in most columns is identical, for those we can use 'mean' or 'first'.\n",
    "\n",
    "For some, like the endangered species columns, only one row will contain our fish, rest will be zero. We must therefore use 'sum' to capture the weight of the endangereed species.\n",
    "\n",
    "For 'Rundvekt' we should also use 'sum'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:31.552691Z",
     "start_time": "2024-04-29T09:41:31.541628Z"
    }
   },
   "outputs": [],
   "source": [
    "species_dict = {\n",
    "    \"Uer (vanlig)\": \"sum\",\n",
    "    \"Blålange\": \"sum\",\n",
    "}\n",
    "tool_dict= dict.fromkeys(['Annet','DivBunntrål','DivSnurrevad','DivSnurpenot/ringnot','DivFlytetrål','DivGarn','DivLine'], \"first\")\n",
    "\n",
    "catch_dict = {\n",
    "        \"Varighet\": \"mean\",\n",
    "        \"Fangstår\": \"mean\",\n",
    "        \"Trekkavstand\": \"mean\",\n",
    "        \"Bredde\": \"mean\",\n",
    "        \"Fartøylengde\" : \"mean\",\n",
    "        \"Bruttotonnasje\": \"mean\",\n",
    "        \"Rundvekt\": \"sum\",\n",
    "        \"Havdybde start\": \"mean\",\n",
    "        \"Havdybde stopp\": \"mean\",\n",
    "        \"Startposisjon lengde\": \"mean\",\n",
    "        \"Startposisjon bredde\": \"mean\",\n",
    "        \"Stopposisjon lengde\": \"mean\",\n",
    "        \"Stopposisjon bredde\": \"mean\",\n",
    "\n",
    "    }\n",
    "master_dict = {**catch_dict, **tool_dict, **species_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It's natural to think that the size of the vessel is important for predicting how much fish is caught.\n",
    "\n",
    "In order to use 'Bruttotonnasje' one option is to pick one of the columns, delete the other one and drop all n/a values.\n",
    "\n",
    "We decided on a slightly different approach in order to preserve more rows.\n",
    "We base the 'Bruttotonnasje' value on the standard 'Bruttotonnasje 1969' and instead of dropping rows right away, we check if the row contains a value in 'Bruttotonnasje annen', and use that.\n",
    "Rows with missing values in both columns are dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:32.510722Z",
     "start_time": "2024-04-29T09:41:32.458040Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.dropna(subset=[\"Bruttotonnasje 1969\", \"Bruttotonnasje annen\"], how='all')\n",
    "df[\"Bruttotonnasje\"] = df[\"Bruttotonnasje 1969\"].fillna(df[\"Bruttotonnasje annen\"])\n",
    "df = df.drop([\"Bruttotonnasje 1969\", \"Bruttotonnasje annen\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "It should be noted that 'Bruttotonnasje' is pretty well explained by 'Fartøylengde', although in a non-linear fashion. In light of this, we could probably omit either 'Bruttotonnasje' or the boat dimensions.\n",
    "See graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:33.708797Z",
     "start_time": "2024-04-29T09:41:33.316072Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df.plot(kind = 'scatter', x = 'Fartøylengde', y = 'Bruttotonnasje', c=\"#43ff640d\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:34.251423Z",
     "start_time": "2024-04-29T09:41:33.992102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df.plot(kind = 'scatter', x = 'Fartøylengde', y = 'Uer (vanlig)', c=\"#888\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:35.301253Z",
     "start_time": "2024-04-29T09:41:34.939584Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df.plot(kind = 'scatter', x = 'Fartøylengde', y = 'Blålange', c=\"#888\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some catch operations are claimed to have taken place in very unusual and sometimes impossible locations.\n",
    "Furthermore, some weights of the endangered species are incredibly high. We suspect this will distort the model, so we remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:37.534857Z",
     "start_time": "2024-04-29T09:41:37.361505Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[df[\"Startposisjon lengde\"] < -50].index)\n",
    "df = df.drop(df[df[\"Startposisjon lengde\"] > 60].index)\n",
    "df = df.drop(df[df[\"Startposisjon bredde\"] < 40].index)\n",
    "df = df.drop(df[df[\"Uer (vanlig)\"] > 25000].index) # Outlier\n",
    "df = df.drop(df[df[\"Blålange\"] > 8000].index) # Outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic Visualization of where Uer is caught surrounding Norway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:38.914619Z",
     "start_time": "2024-04-29T09:41:38.557890Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize an axis\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# plot map on axis\n",
    "countries = gpd.read_file(  \n",
    "     gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "countries.plot(color=\"lightgrey\",\n",
    "                                                 ax=ax)\n",
    "df_positive_uer = df.drop(df[df[\"Uer (vanlig)\"] == 0].index)\n",
    "\n",
    "# plot points\n",
    "df_positive_uer.plot(x=\"Startposisjon lengde\", y=\"Startposisjon bredde\", kind=\"scatter\", \n",
    "        c=\"Uer (vanlig)\", colormap=\"YlOrRd\", s=df_positive_uer[\"Uer (vanlig)\"].multiply(0.003),\n",
    "        ax=ax)\n",
    "# add grid\n",
    "ax.grid( alpha=0.5)\n",
    "ax.set_xlim(0, 40)\n",
    "ax.set_ylim(55, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Geographic Visualization of where Blålange is caught surrounding Norway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:40.040821Z",
     "start_time": "2024-04-29T09:41:39.854174Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize an axis\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# plot map on axis\n",
    "countries = gpd.read_file(  \n",
    "     gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "countries.plot(color=\"lightgrey\",\n",
    "                                                 ax=ax)\n",
    "\n",
    "df_positive_lange = df.drop(df[df[\"Blålange\"] == 0].index)\n",
    "# plot points\n",
    "df_positive_lange.plot(x=\"Startposisjon lengde\", y=\"Startposisjon bredde\", kind=\"scatter\", \n",
    "        c=\"Blålange\", colormap=\"YlOrRd\", s=df_positive_lange[\"Blålange\"].multiply(0.04),\n",
    "        ax=ax)\n",
    "# add grid\n",
    "ax.grid( alpha=0.5)\n",
    "ax.set_xlim(-20, 35)\n",
    "ax.set_ylim(50, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to collapse the entries into catch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:41.566232Z",
     "start_time": "2024-04-29T09:41:41.390653Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_processed = df.groupby(by=[\"Melding ID\", \"Starttidspunkt\", \"Stopptidspunkt\"]).agg(master_dict).reset_index()\n",
    "df_processed = df_processed.drop(columns=['Starttidspunkt','Stopptidspunkt',\"Melding ID\"])\n",
    "df_processed = df_processed.dropna()\n",
    "\n",
    "df_knn = df_processed.copy()\n",
    "df_dl = df_processed.copy()\n",
    "df_xg = df_processed.copy()\n",
    "df_clustering = df_processed.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "***KNN Regressor model***\n",
    "\n",
    "\n",
    "A k-nearest neighbors (KNN) regressor is a non-parametric machine learning model used for regression tasks. It predicts the value of a target variable by averaging the values of the k-nearest neighbors in the feature space.\n",
    "\n",
    "Because of few observations of Blålange we focus on predicting Uer (vanlig).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:41:44.564174Z",
     "start_time": "2024-04-29T09:41:44.546335Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_knn.drop('Uer (vanlig)', axis=1)\n",
    "y = df_knn['Uer (vanlig)']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The code under performs a grid search to optimize the hyperparameters of a KNN regressor model using 5-fold cross-validation. It searches through a predefined set of possible values for the number of neighbors (\"n_neighbors\") and the weighting function (\"weights\") to find the combination that minimizes the negative mean squared error. This approach is used to systematically work through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:42:09.089758Z",
     "start_time": "2024-04-29T09:41:48.035595Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getBestRegressor():\n",
    "    # Defining parameter grid\n",
    "    param_grid = {'n_neighbors': [3, 4, 5, 6, 7, 10, 15, 20], 'weights': ['uniform', 'distance']}\n",
    "    grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # finding the best parameters\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    best_knn_regressor = grid_search.best_estimator_\n",
    "    return best_knn_regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:42:13.788615Z",
     "start_time": "2024-04-29T09:42:13.059722Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# knn_regressor = getBestRegressor() # Function takes very long time to run.\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3, weights=\"distance\")\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "y_pred = knn_regressor.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:42:15.378688Z",
     "start_time": "2024-04-29T09:42:15.357626Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "MSE: Mean Squared Error is a measure of the average squared difference between the observed actual amount of Uer (vanlig) and the  predicted amount of Uer (vanlig) by the model, indicating a value of 122,397.15 in this case.\n",
    "\n",
    "RMSE: Root Mean Squared Error is the square root of MSE, providing a measure of the average magnitude of the errors, with a value of 349.85 suggesting the typical prediction error's size.\n",
    "\n",
    "MAE: Mean Absolute Error (MAE) is the average of the absolute differences between predicted values of Uer (vanlig) and actual values, with a value of 49.21 indicating the average prediction error in the same units as the data.\n",
    "\n",
    "R^2 Score: The R-squared score, often called the coefficient of determination, is the proportion of the variance in the dependent variable that is predictable from the independent variables, with a value of 0.52 indicating that approximately 52.19% of the variability in the outcome is explained by the model.\n",
    "\n",
    "We now want to visualize the predictions made by our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:42:19.271931Z",
     "start_time": "2024-04-29T09:42:19.156921Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotPredictions(test,pred):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(test, pred, alpha=0.5)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Real values (log scale)')\n",
    "    plt.ylabel('Predicted values (log scale)')\n",
    "    plt.title('Real values vs. Predicted values for Uer (log scale)')\n",
    "\n",
    "    plt.plot([min(test), max(test)], [min(test), max(test)], 'k--')\n",
    "    plt.show()\n",
    "\n",
    "plotPredictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log graph indicates that the model is capable of predicting the correct values to a certain extent, as evidenced by the clustering of data points along the line of perfect prediction, although there is still some deviation present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:42:25.252009Z",
     "start_time": "2024-04-29T09:42:25.086541Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals = residuals[~np.isnan(residuals)]\n",
    "residuals = residuals[np.isfinite(residuals)]\n",
    "\n",
    "# Distribution of 10% random sample of the data resudials\n",
    "test_size = int(len(residuals) * 0.1)\n",
    "random_sample = np.random.choice(residuals, size=test_size, replace=False)\n",
    "\n",
    "plt.xlim(-800, 1500)\n",
    "sns.histplot(random_sample, kde=True)\n",
    "plt.title('Distribution of the residuals (10% random samle)')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals being centered around zero suggests that the model's predictions are, on average, accurate, as there is no systematic overprediction or underprediction. This indicates a good fit between the model and the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tensorflow deep learning model***\n",
    "\n",
    "A TensorFlow deep learning model is a computational architecture composed of multiple layers of interconnected nodes, or \"neurons,\" designed to recognize patterns in data by adjusting internal parameters through a process known as training. TensorFlow is an open-source machine learning library developed by Google that facilitates the creation, training, and deployment of such models for a wide variety of complex tasks, including image and speech recognition, natural language processing, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:42:56.123085Z",
     "start_time": "2024-04-29T09:42:43.046933Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "min_val = df_dl['Uer (vanlig)'].min() # null\n",
    "max_val = df_dl['Uer (vanlig)'].max()\n",
    "bands = [min_val, 10, 100 , max_val]\n",
    "\n",
    "print(min_verdi)\n",
    "print(max_verdi)\n",
    "kategorier = ['few', 'some', 'many']\n",
    "df_dl['Uer_kategorisert'] = pd.cut(df_dl['Uer (vanlig)'], bins=bands , labels=kategorier, include_lowest=True)\n",
    "\n",
    "y = df_dl['Uer_kategorisert']\n",
    "y = y.astype('category').cat.codes\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Too few non-zero observations, need to weigh the observations we do have.\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_encoded, epochs=10, class_weight=class_weights_dict, validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:43:01.148742Z",
     "start_time": "2024-04-29T09:43:00.747408Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test_encoded, y_pred_classes) \n",
    "print(\"Forvirringsmatrise: \\n\", conf_mat)\n",
    "\n",
    "class_report = classification_report(y_test_encoded, y_pred_classes, target_names=['few', 'some', 'many'])  \n",
    "print(\"\\nKlassifiseringsrapport: \\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***XGBoost Algo***\n",
    "\n",
    "\n",
    "XGBoost, short for eXtreme Gradient Boosting, is an ensemble machine learning algorithm known for its efficiency and scalability in training models. It enhances accuracy by combining the predictions of several simpler models into a powerful composite predictor. \n",
    "\n",
    "The algorithm has a track record of winning numerous machine learning contests, making it not only a sound choice for the project but also a trendy one. One of XGBoost's key features is its efficient handling of missing data, which allows it to manage real-world datasets with missing values without great preprocessing. \n",
    "\n",
    "Incorporating XGBoost as one of our supervised learning models aligns with the precision required for our hypothetical use case and adds a touch of trendiness and spiciness to the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:44:56.798623Z",
     "start_time": "2024-04-29T09:44:56.722540Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_xgboost = df_xgboost.drop(\"Blålange\", axis = 1)\n",
    "\n",
    "# Defining the independent variables (X) and the dependent varaible (y).\n",
    "y = df_xgboost['Uer (vanlig)']\n",
    "X = df_xgboost.drop('Uer (vanlig)', axis=1)\n",
    "\n",
    "# Splitting the dataset into training and test set with a 70-30 split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "df_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Model Tuning and Hyperparamterer Optimalization with Grid Search:\n",
    "\n",
    "After splitting the dataset, it is important to perform fine-tuning on our model's hyperparameters. Tuning can ensure that we enhance the model's performance. We use Grid Search for this purpose, which methodically explores a range of possibilities for hyperparameters to find the best combinations. This process is computationally demanding, but it is crucial to avoid overlooking more optimized settings for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:44:57.741769Z",
     "start_time": "2024-04-29T09:44:57.717379Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculateOptimizedParameters():\n",
    "    # Defining a reasonable paramterer grid or distribution to sample from.\n",
    "    param_grid = {\n",
    "        'colsample_bytree': [0.3, 0.5, 0.7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'alpha': [1, 10, 50],\n",
    "        'n_estimators': [200, 800, 1500]\n",
    "    }\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "    grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# calculateOptimizedParameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Result of Grid Search\n",
    "\n",
    "The Grid search is a computationally demandning process and it can be time-consuming.\n",
    "On my computer, the search took about 15 minutes. Therefore, I have included the results here.\n",
    "The optimal hyperparamters identified by Grid Search for our XGBoost model are as follows:\n",
    "\n",
    "alpha: 10\n",
    "\n",
    "colsample_bytree: 0.5\n",
    "\n",
    "learning_rate: 0.1\n",
    "\n",
    "max_depth: 7\n",
    "\n",
    "n_estimators: 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:09.697678Z",
     "start_time": "2024-04-29T09:45:01.163327Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializing the the XGBoost regressor with hyperparamters.\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "                        colsample_bytree = 0.7, \n",
    "                        learning_rate = 0.1,\n",
    "                        max_depth = 7, \n",
    "                        alpha = 10, \n",
    "                        n_estimators =800)\n",
    "\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Evaluering av modellen\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE (Mean Squared Error): {mse}\")\n",
    "print(f\"RMSE (Root Mean Squared Error): {rmse}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Feature Analysis\n",
    "\n",
    "Before we examine the model's predictions in detail, it is important to understand the influence of different features on the model's output. Feature analysis helps us identify which features significantly impact the predictions and provides insights into the relationships within our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:20.205954Z",
     "start_time": "2024-04-29T09:45:20.081199Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Visualizing the Predictions\n",
    "\n",
    "After examining the importance of different features, we will now provide a visualization of the model's predictions. By plotting actual values against predicted values, we can visualize the model's accuracy. The plot helps us visualize how closely the model's predictions align with the ideal representation of perfect agreement between predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:22.270076Z",
     "start_time": "2024-04-29T09:45:22.151625Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plotPredictions(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Residual Analysis\n",
    "\n",
    "Residual analysis is used in evaluation a regression models performance. The analysis examines the difference between actual and predicted values. Randomly distributed residuals tend to suggest a well-fitting model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:23.956816Z",
     "start_time": "2024-04-29T09:45:23.861594Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.hlines(y=0, xmin=y_test.min(), xmax=y_test.max(), colors='r', linestyles='--')\n",
    "plt.xlabel('Faktiske verdier')\n",
    "plt.ylabel('Residualer')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "***K-Means Clustering***\n",
    "\n",
    "is an unsupervised machine learning algorithm used to identify and group similar data points into clusters. The algorithm partitions the data points into K distinct clusters, aiming to minimize the variance within each cluster and maximize the variance between clusters. We use K-Means to detect patterns that might be hidden within our data, without the need to predefine labels.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:26.098082Z",
     "start_time": "2024-04-29T09:45:26.026654Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_columns = [\"Startposisjon bredde\", \"Startposisjon lengde\", \n",
    "    \"Havdybde start\", \"Varighet\", \"Fangstår\", \n",
    "    \"Stopposisjon bredde\", \"Stopposisjon lengde\", \n",
    "    \"Havdybde stopp\", \"Trekkavstand\", \"Rundvekt\", \n",
    "    \"Bredde\", \"Fartøylengde\", \"Bruttotonnasje\"]\n",
    "\n",
    "df_clustering = df_clustering[relevant_columns]\n",
    "scaler = StandardScaler()\n",
    "df_clustering_scaled = scaler.fit_transform(df_clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:27.899163Z",
     "start_time": "2024-04-29T09:45:27.879056Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***The Elbow Method***\n",
    "\n",
    "To effectively utilize K-Means clustering, it is important to first determine an appropriate number of clusters. We have chosen the Elbow Method to find this number by calculating the inertia for different values of K.\n",
    "\n",
    "Inertia measures the sum of squared distances of samples to their nearest cluster center. By plotting inertia values for different values of K, we can identify what is known as the \"Elbow Point\". At this point, the rate of decrease in the graph sharply changes.\n",
    "\n",
    "This point suggests a balance between the number of clusters and the compactness of the clusters, which in turn indicates a suitable value for K in K-Means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:35.509139Z",
     "start_time": "2024-04-29T09:45:29.573693Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implementing the Elbow Method. \n",
    "\n",
    "inertia = []\n",
    "\n",
    "# Calculating the intertia for a range of clusters (k).\n",
    "for k in range(1, 21):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42) \n",
    "    kmeans.fit(df_clustering_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 21), inertia)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After plotting the graph, we find that a suitable value for K in K-means is K = 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:39.988806Z",
     "start_time": "2024-04-29T09:45:39.711134Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(df_clustering_scaled)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "df_clustering_2 = df_processed.copy()\n",
    "df_clustering_2[\"cluster\"] = labels\n",
    "\n",
    "for i in range(k):\n",
    "    cluster_data = df_clustering_2[df_clustering_2[\"cluster\"] == i]\n",
    "    print(f\"Cluster {i}:\")\n",
    "    # Sørg for at kolonnene 'Uer (vanlig)' og 'Blålange' eksisterer i df_processed før du prøver å få tilgang til dem\n",
    "    if 'Uer (vanlig)' in df_clustering_2.columns and 'Blålange' in df_clustering_2.columns:\n",
    "        print(f\"Vanlig Uer: {cluster_data['Uer (vanlig)'].mean()}\")\n",
    "        print(f\"Blålange: {cluster_data['Blålange'].mean()}\")\n",
    "    else:\n",
    "        print(\"En eller begge kolonnene 'Uer (vanlig)' og 'Blålange' finnes ikke i DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Principle Component Analysis***\n",
    "\n",
    "We utilize a technique known as Prinicple Component Analysis (PCA) to reduce the dimensionality of the dataset by transforming into principal components. These orthogonal, uncorrelated components captures the greatest variance in the dataset, across fewer dimensions. \n",
    "\n",
    "PCA enables us to project the data into a lower-dimensional plot to visualize the clusters. In our task, we have used PCA to facilitate a 2D plot, which allows for the interpretation of cluster groupings and visualize separations that might be obscured in higher-dimensional spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T09:45:44.532759Z",
     "start_time": "2024-04-29T09:45:41.856978Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reducing to 2 dimensions and transforming the scaled dataset to find the components that best captures the variance.\n",
    "pca = PCA(n_components=2)  \n",
    "principal_components = pca.fit_transform(df_clustering_scaled)\n",
    "\n",
    "# Creating a dataframe with the components to plot it in a 2D space.\n",
    "df_visual = pd.DataFrame(data=principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "df_visual[\"cluster\"] = labels  \n",
    "\n",
    "# Creating a scatter plot to visualize the clusters.\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(k):\n",
    "    cluster_subset = df_visual[df_visual['cluster'] == i]\n",
    "    plt.scatter(cluster_subset['PC1'], cluster_subset['PC2'], label=f'Cluster {i}')\n",
    "\n",
    "plt.title('2D PCA Clustering Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:22:01.572408Z",
     "start_time": "2024-04-27T14:22:00.700318Z"
    },
    "collapsed": false
   },
   "source": [
    "***Analyzing Cluster Centroid Properties***\n",
    "\n",
    "Analyzing the properties of cluster centroids allows us to gain important insights into unique clusters characteristics and defining traits. \n",
    "This understanding can reveal underlying patterns within our dataset, providing context for the cluster behaviour. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentroid for klynge 0:\n",
      "Startposisjon bredde       59.874907\n",
      "Startposisjon lengde        4.462447\n",
      "Havdybde start           -188.482127\n",
      "Varighet                  611.972271\n",
      "Fangstår                 2018.000000\n",
      "Stopposisjon bredde        59.870554\n",
      "Stopposisjon lengde         4.510221\n",
      "Havdybde stopp           -188.394671\n",
      "Trekkavstand            18072.352289\n",
      "Rundvekt                 8485.358451\n",
      "Bredde                      7.914817\n",
      "Fartøylengde               30.136005\n",
      "Bruttotonnasje            345.719674\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Sentroid for klynge 1:\n",
      "Startposisjon bredde      71.034736\n",
      "Startposisjon lengde      22.385406\n",
      "Havdybde start          -183.021476\n",
      "Varighet                 321.335265\n",
      "Fangstår                2018.000000\n",
      "Stopposisjon bredde       71.036953\n",
      "Stopposisjon lengde       22.415618\n",
      "Havdybde stopp          -181.138486\n",
      "Trekkavstand            5747.265326\n",
      "Rundvekt                9273.232316\n",
      "Bredde                     7.921340\n",
      "Fartøylengde              30.104365\n",
      "Bruttotonnasje           364.140037\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Sentroid for klynge 2:\n",
      "Startposisjon bredde       74.013188\n",
      "Startposisjon lengde       28.195496\n",
      "Havdybde start           -249.782927\n",
      "Varighet                 1627.220359\n",
      "Fangstår                 2018.000000\n",
      "Stopposisjon bredde        74.026483\n",
      "Stopposisjon lengde        28.212087\n",
      "Havdybde stopp           -252.486585\n",
      "Trekkavstand            12259.399407\n",
      "Rundvekt                10444.529505\n",
      "Bredde                     13.727338\n",
      "Fartøylengde               63.881856\n",
      "Bruttotonnasje           2387.890447\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Sentroid for klynge 3:\n",
      "Startposisjon bredde        61.396602\n",
      "Startposisjon lengde        -4.693854\n",
      "Havdybde start           -2186.346980\n",
      "Varighet                   266.137793\n",
      "Fangstår                  2018.000000\n",
      "Stopposisjon bredde         61.188758\n",
      "Stopposisjon lengde         -4.686979\n",
      "Havdybde stopp           -2165.079381\n",
      "Trekkavstand             37394.552671\n",
      "Rundvekt                298510.800799\n",
      "Bredde                      13.327918\n",
      "Fartøylengde                66.206625\n",
      "Bruttotonnasje            1988.150774\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Sentroid for klynge 4:\n",
      "Startposisjon bredde       74.90225\n",
      "Startposisjon lengde       15.93925\n",
      "Havdybde start           -381.75000\n",
      "Varighet                  280.00000\n",
      "Fangstår                 2017.00000\n",
      "Stopposisjon bredde        74.91625\n",
      "Stopposisjon lengde        15.96325\n",
      "Havdybde stopp           -366.25000\n",
      "Trekkavstand             7123.75000\n",
      "Rundvekt                16538.50000\n",
      "Bredde                     12.60000\n",
      "Fartøylengde               56.80000\n",
      "Bruttotonnasje           1476.00000\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "Sentroid for klynge 5:\n",
      "Startposisjon bredde       61.576475\n",
      "Startposisjon lengde        0.678918\n",
      "Havdybde start           -234.116523\n",
      "Varighet                  273.582047\n",
      "Fangstår                 2018.000000\n",
      "Stopposisjon bredde        61.579020\n",
      "Stopposisjon lengde         0.697551\n",
      "Havdybde stopp           -237.448889\n",
      "Trekkavstand            13218.944216\n",
      "Rundvekt                41171.434730\n",
      "Bredde                     13.475928\n",
      "Fartøylengde               64.996690\n",
      "Bruttotonnasje           2105.435580\n",
      "Name: 5, dtype: float64\n",
      "\n",
      "Cluster 0:\n",
      "Vanlig Uer: 6.776144366197183\n",
      "Blålange: 8.476452464788732\n",
      "Cluster 1:\n",
      "Vanlig Uer: 26.4711676095699\n",
      "Blålange: 0.11836275645831913\n",
      "Cluster 2:\n",
      "Vanlig Uer: 121.96906657631713\n",
      "Blålange: 0.027308148399119093\n",
      "Cluster 3:\n",
      "Vanlig Uer: 0.4805389221556886\n",
      "Blålange: 0.0\n",
      "Cluster 4:\n",
      "Vanlig Uer: 9.25\n",
      "Blålange: 0.0\n",
      "Cluster 5:\n",
      "Vanlig Uer: 43.47617361750336\n",
      "Blålange: 12.810946682716137\n"
     ]
    }
   ],
   "source": [
    "# Gathering the cluster centers identifed by K-means and creating a dataframe.\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids_df = pd.DataFrame(scaler.inverse_transform(centroids), columns=relevant_columns)\n",
    "\n",
    "# Printing the centroids of each cluster.\n",
    "for i in range(k):\n",
    "    print(f\"Sentroid for klynge {i}:\")\n",
    "    print(centroids_df.loc[i])\n",
    "    print()\n",
    "\n",
    "# Analyzing and printing important information about clusters.\n",
    "for i in range(k):\n",
    "    cluster_data = df_processed_dropna[df_processed_dropna[\"cluster\"] == i]\n",
    "    print(f\"Cluster {i}:\")\n",
    "    if 'Uer (vanlig)' in df_processed_dropna.columns and 'Blålange' in df_processed_dropna.columns:\n",
    "        print(f\"Vanlig Uer: {cluster_data['Uer (vanlig)'].mean()}\")\n",
    "        print(f\"Blålange: {cluster_data['Blålange'].mean()}\")\n",
    "    else:\n",
    "        print(\"Blålange and Uer does not exists in one or both columns of the dataframe\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tabular Comparison of Clusters***\n",
    "\n",
    "Creating a tabular comparison to better visualize, simplify the data, to facilitate a more intuitive analysis in order to draw meaningful conclusions from our clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Startposisjon bredde  Startposisjon lengde  Havdybde start  \\\n",
      "0                        59.874907              4.462447     -188.482127   \n",
      "1                        71.034736             22.385406     -183.021476   \n",
      "2                        74.013188             28.195496     -249.782927   \n",
      "3                        61.396602             -4.693854    -2186.346980   \n",
      "4                        74.902250             15.939250     -381.750000   \n",
      "5                        61.576475              0.678918     -234.116523   \n",
      "Overall Mean             67.774773             16.279589     -252.924540   \n",
      "\n",
      "                 Varighet     Fangstår  Stopposisjon bredde  \\\n",
      "0              611.972271  2018.000000            59.870554   \n",
      "1              321.335265  2018.000000            71.036953   \n",
      "2             1627.220359  2018.000000            74.026483   \n",
      "3              266.137793  2018.000000            61.188758   \n",
      "4              280.000000  2017.000000            74.916250   \n",
      "5              273.582047  2018.000000            61.579020   \n",
      "Overall Mean   775.420017  2017.999959            67.774550   \n",
      "\n",
      "              Stopposisjon lengde  Havdybde stopp  Trekkavstand  \\\n",
      "0                        4.510221     -188.394671  18072.352289   \n",
      "1                       22.415618     -181.138486   5747.265326   \n",
      "2                       28.212087     -252.486585  12259.399407   \n",
      "3                       -4.686979    -2165.079381  37394.552671   \n",
      "4                       15.963250     -366.250000   7123.750000   \n",
      "5                        0.697551     -237.448889  13218.944216   \n",
      "Overall Mean            16.307617     -253.200868  12309.718066   \n",
      "\n",
      "                   Rundvekt     Bredde  Fartøylengde  Bruttotonnasje  \\\n",
      "0               8485.358451   7.914817     30.136005      345.719674   \n",
      "1               9273.232316   7.921340     30.104365      364.140037   \n",
      "2              10444.529505  13.727338     63.881856     2387.890447   \n",
      "3             298510.800799  13.327918     66.206625     1988.150774   \n",
      "4              16538.500000  12.600000     56.800000     1476.000000   \n",
      "5              41171.434730  13.475928     64.996690     2105.435580   \n",
      "Overall Mean   19985.171122  10.588031     46.102431     1256.364658   \n",
      "\n",
      "              Vanlig Uer   Blålange  \n",
      "0               6.776144   8.476452  \n",
      "1              26.471168   0.118363  \n",
      "2             121.969067   0.027308  \n",
      "3               0.480539   0.000000  \n",
      "4               9.250000   0.000000  \n",
      "5              43.476174  12.810947  \n",
      "Overall Mean   52.663057   3.866551  \n"
     ]
    }
   ],
   "source": [
    "# Calculating mean of the dataset as refernce.\n",
    "overall_means = df_clustering.mean()\n",
    "\n",
    "# Converting centroids to dataframe for analyzing purposes in tabular form.\n",
    "centroids_df = pd.DataFrame(scaler.inverse_transform(centroids), columns=relevant_columns)\n",
    "\n",
    "# Adding mean values of the endangered species.\n",
    "cluster_means = []\n",
    "for i in range(k):\n",
    "    cluster_data = df_processed_dropna[df_processed_dropna[\"cluster\"] == i]\n",
    "    cluster_means.append({\n",
    "        'Vanlig Uer': cluster_data['Uer (vanlig)'].mean(),\n",
    "        'Blålange': cluster_data['Blålange'].mean()\n",
    "    })\n",
    "\n",
    "# Converting cluster means to a dataframe.\n",
    "cluster_means_df = pd.DataFrame(cluster_means)\n",
    "\n",
    "# Combinging the dataframes.\n",
    "comparison_df = pd.concat([centroids_df, cluster_means_df], axis=1)\n",
    "\n",
    "# Adding the mean values.\n",
    "comparison_df.loc['Overall Mean'] = overall_means.tolist() + [df_processed_dropna['Uer (vanlig)'].mean(), df_processed_dropna['Blålange'].mean()]\n",
    "\n",
    "# Printing the tables.\n",
    "print(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Did We learn from Clustering?\n",
    "\n",
    "Based on the results from the clustering model, several observations can be made regarding the variability between the clusters, geographical distribution, catch operating vessel characteristics, duratio and wather depth.\n",
    "\n",
    "***Variability In Clusters:***\n",
    "\n",
    "Clusters 2, 3 and 4 predominantly report catches of Uer, with a negligable amount of Blålange.\n",
    "Clusters 1 and 2 contraints both species. However, cluster 0 has approximately three times more Uer than Blålange, while cluster 0 has a more baanced amount of both.\n",
    "Cluster 4 and 5 recorded almost no catches of both endangered species. Cluster 4 has an average of 9 caught Uer, and barely any Blålange, while cluster 5 has minimal catches of both.\n",
    "\n",
    "***Geographical Insights:***\n",
    "\n",
    "Clusters 0, 1 and 4 are distinct from clusters 2, 3 and 5. The separation in geographical areas may contribute to differences in species composition, suggesting that some areas may have a higher presence of Uer and Blålange.\n",
    "\n",
    "***Catch Operation Duration:***\n",
    "\n",
    "Most clusters exhibit similar durations for catch operations, with the exception of cluster 2, which has a significantly longer duration. This cluster also has a higher mean catch of Vanlig Uer, which is substantially greater than the second-highest mean catch recorded by cluster 0. This indicates that a longer catch operation, may contribute to catching more endangered species.\n",
    "\n",
    "***Water Depth:***\n",
    "\n",
    "Cluster 5 operates at a significantly lower water depth compared to the other clusters and the overall mean, almost 11 times deeper. The negligible catch of Blålange or Uer in this cluster may indicate that these species do not thrive at such deep waters.\n",
    "\n",
    "***Vessel Characteristics:***\n",
    "\n",
    "Vessel length, vessel width, whole fish weight, and gross tonnage reveals that clusters 1 and 3 have lower and similar values compared to the other clusters and the overall means. These clusters reports low catches of endangered species, which might suggest that smaller vessels, catch less fish, and consequently, less endangered fish. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-27T14:22:01.843788Z"
    }
   },
   "source": [
    "**Results and comments**\n",
    "\n",
    "TODO: \"write a summary of your results, and discuss consequences of such results.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
